# Решение.
На каждый пункт анкеты построим классификатор токена начала и классификатор токена конца. Всего 4 классификатора.  
В качестве модели будем использовать `LSTM`. На вход модели подаются последовательности одинаковой длины $N$. Для каждого элемента последовательности берется выход `LSTM` (размера скрытого состояния), проектируется полносвязным в одно число число. Таким образом, получается вектор из $N$ чисел. Далее этот вектор отправляется в `Softmax` слой, чтобы получить распределение вероятностьей. Последний токен в последовательности соответствует тому случаю, когда в тексте нет соответствующего пункта анкеты.  
Было опробовано 2 способа извлечения эмбеддингов слов: обучение `Embedding` слоя и предобученные `Glove` вектора из проекта [Navec](https://natasha.github.io/navec/). В итоге остановился на первом варианте.  
При генерации посылки произошла неожиданность. Возникли проблемы с тем, чтобы однозначно восстановить по индексам токенов последовательности изначальный текст. Пришлось руками делать исправления сконструированного текста при помощи простых замен и регулярных выражений (см. ноутбук `submission.ipynb`), что снизило итоговое качество (на валидации при сравнении последовательностей токенов она получилось больше, см. тот же ноутбук), к тому же индексы в изначальном тексте удалось восстановить совсем плохо.

# Содержание.
* `data` - папка с данными, разбитыми на обучение и валидацию по пунктам анкеты
* `experiments` - нектороые эксперименты с обучением и с моделями
* `data_preparation.ipynb` - подготовка данных для обучения
* `document_dataset.py` - представление данных
* `eda.ipynb` - немного анализа имеющихся данных
* `models.py` - опробованные модели
* `submission.ipynb` - ноутбук для подготовки посылки

# Что хотелось сделать еще
Хотел еще попробовать другой подход: обучить по одному классификатору на каждый пункт анкеты, чтобы они предсказывали распределение (`начало`, `конец`, `ничего`) для каждого токена отдельно. Но руки до этого не дошли.