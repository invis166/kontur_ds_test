{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.cwd().parent/'dataset/train.json') as f:\n",
    "    train_documents = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на обучение и валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [doc['label'] for doc in train_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "indices = np.arange(len(labels))\n",
    "_, _, train_indices, test_indices = train_test_split(labels, indices, test_size=0.25, random_state=RANDOM_STATE)\n",
    "# TODO: add stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = []\n",
    "test_split = []\n",
    "\n",
    "for train_idx in train_indices:\n",
    "    train_split.append(train_documents[train_idx])\n",
    "\n",
    "for test_idx in test_indices:\n",
    "    test_split.append(train_documents[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_split.json', 'w') as train_f, open('test_split.json', 'w') as test_f:\n",
    "    json.dump(train_split, train_f)\n",
    "    json.dump(test_split, test_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим словарь токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_garbage = [\n",
    "    'arbitr',\n",
    "    'ast',\n",
    "    'c',\n",
    "    'doc',\n",
    "    'docx',\n",
    "    'e',\n",
    "    'footnoteref',\n",
    "    'gov',\n",
    "    'http',\n",
    "    'i',\n",
    "    'ii',\n",
    "    'iii',\n",
    "    'iv',\n",
    "    'mail',\n",
    "    'mailto',\n",
    "    'n',\n",
    "    'page',\n",
    "    'rosatom',\n",
    "    'roseltorg',\n",
    "    'rt',\n",
    "    'ru',\n",
    "    'sberbank',\n",
    "    'tender',\n",
    "    'unknown',\n",
    "    'v',\n",
    "    'vii',\n",
    "    'viii',\n",
    "    'word',\n",
    "    'www',\n",
    "    'yandex',\n",
    "    'zakupki',\n",
    "    'zi'\n",
    "]\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "def filter_words(token_counter, min_count):\n",
    "    filtered_words = sorted([\n",
    "        word\n",
    "        for word in token_counter\n",
    "        if (token_counter[word] >= min_count and word.isalpha() and word not in stop_words and word not in collected_garbage)\n",
    "    ])\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "def _make_word_idx_map(documents: dict, min_count=11) -> dict:\n",
    "    token_counter = defaultdict(int)\n",
    "    stemmer = PorterStemmer()\n",
    "    for entry in documents:\n",
    "        for token in wordpunct_tokenize(entry['text']):\n",
    "            token = stemmer.stem(token)\n",
    "            token_counter[token] += 1\n",
    "\n",
    "    filtered_words = filter_words(token_counter, min_count)\n",
    "\n",
    "    word_idx = dict(zip(filtered_words, range(2, len(filtered_words) + 2)))\n",
    "    word_idx['PAD'] = 0\n",
    "    word_idx['UNK'] = 1\n",
    "\n",
    "    return word_idx, token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx, token_counter = _make_word_idx_map(train_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_idx.json', 'w') as f:\n",
    "    json.dump(word_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2432"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем разбиение по названиям пункта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_split.json') as train_f, open('test_split.json') as test_f:\n",
    "    train_split = json.load(train_f)\n",
    "    test_split = json.load(test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label_train = [doc for doc in train_split if doc['label'] == 'обеспечение исполнения контракта']\n",
    "second_label_train = [doc for doc in train_split if doc['label'] != 'обеспечение исполнения контракта']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 619)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_label_train), len(second_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label_test = [doc for doc in test_split if doc['label'] == 'обеспечение исполнения контракта']\n",
    "second_label_test = [doc for doc in test_split if doc['label'] != 'обеспечение исполнения контракта'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258, 192)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_label_test), len(second_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_label_train.json', 'w') as first_train, \\\n",
    "    open('second_label_train.json', 'w') as second_train, \\\n",
    "    open('first_label_test.json', 'w') as first_test, \\\n",
    "    open('second_label_test.json', 'w') as second_test:\n",
    "    json.dump(first_label_train, first_train)\n",
    "    json.dump(second_label_train, second_train)\n",
    "    json.dump(first_label_test, first_test)\n",
    "    json.dump(second_label_test, second_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
